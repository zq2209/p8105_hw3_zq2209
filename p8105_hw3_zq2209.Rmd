---
title: "p8105_hw3_zq2209"
author: "Zining Qi"
date: "2022-10-12"
output: github_document
---
```{r}
library(tidyverse)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


# Problem 1
```{r}
library(p8105.datasets)
data("instacart")
```

```{r}
instacart = 
  instacart %>% 
  as_tibble(instacart)
```

```{r}
# How many aisles are there, and which aisles are the most items ordered from?
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

```{r}
# Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle",
       x = "Aisle",
       y = "Number of items") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

```{r}
# Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

```{r}
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable(digits = 2)
```



# Problem 2
```{r}
accel = read_csv("./dataset/accel_data.csv")
```

```{r}
# Cleaning, tidying, and wrangling data
accel_data = accel %>% 
  janitor::clean_names() %>% 
  drop_na() %>% 
  mutate(Weekday_Weekend = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday")) %>% 
  group_by(week, day_id) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_time", 
    names_prefix = "activity_",
    values_to = "activity_amount")

nrow(accel_data)
ncol(accel_data)

accel_data
```

After tidying,, cleaning and adding variables, there are 50400 rows, and 6 columns. The variables are week, day_id, day, weekday vs weekend, activity time, and activity amount. Activity time is the minute if activity in that day, and activity amount is the count of that activity in that minute.Weekday vs Weekend is whether that day is weekday or weekend.


```{r}
# Total activity over the day
total_activity = accel_data %>% 
  group_by(week, day, day_id) %>% 
  summarize(total = sum(activity_amount)) %>% 
  knitr::kable()
total_activity

total_activity_plot = accel_data %>% 
  group_by(week, day, day_id) %>% 
  summarize(total = sum(activity_amount)) %>% 
  ggplot(aes(x = day, y = total, color = week)) +
  geom_point()
total_activity_plot
```

As shown in the plot, The total amount of activities in Friday, Monday, Saturday, and Sunday are more spread. In Thursday, Tuesday, and Wednesday, total activities are tighter. And in Saturday, it has lowest activity amount.

```{r}
# Activity over the course of the 24 hours
activity_of_day = accel_data %>% 
  mutate(activity_amount = round(activity_amount,0),
         activity_time = as.integer(activity_time)) %>% 
  ggplot(aes(x = activity_time, y = activity_amount, color = day)) +
  geom_smooth(se = FALSE) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("12:00 AM", "4:00 AM", "8:00 AM", "12:00 PM", "4:00 PM", "8:00 PM", "12:00 PM")
  ) +
  labs(
    title = "Activity over the course of the 24 hours",
    x = "Time of Activity",
    y = "Amount of Activity"
  )

activity_of_day
```

According to the graph, activity amout during 12am to 4am is really low, almost 0. And the amount start increasing from 4am. The amount from 8am to 6pm are stay approximate consistent, except for Thursday. There is a peak in 11am on Thursday. And from 8pm to 10pm, the activity amount increase, especially on Friday, the amount is the highest. From 11pm, it start decreasing.



# Problem 3
```{r}
library(p8105.datasets)
data("ny_noaa")
```

```{r}
ny_noaa %>% 
  janitor::clean_names() %>% 
  summary()

nrow(ny_noaa)
ncol(ny_noaa)
```

There are 2595176 rows and 7 columns. The dataset contains 7 variables, which are station id, date, prcp:Precipitation (tenths of mm), snow:Snowfall (mm), snwd:Snow depth (mm), tmax:Maximum temperature (tenths of degrees C), tmin:Minimum temperature (tenths of degrees C). prcp, tmax, tmin are measured in tenth, so it will be divided by 10. But for prcp, snow, and snwd, tmax, and tmin, there are many NA, shown in summary table. NAs have to be dropped during calculation later.


```{r}
# Do some data cleaning
noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         year = as.numeric(year),
         month = as.numeric(month),
         day = as.numeric(day),
         month = month.abb[month],
         tmax = tmax / 10,
         tmin = tmin / 10,
         prcp = prcp / 10
         ) %>% 
  select(year, month, day, everything())

noaa  
```

Setting prcp, tmax, and tmin to standard unit, dividing by 10. Create new variables of year, month, and day.

```{r}
# The most commonly observed values for snowfall
most_observed_snow = noaa %>% 
  count(snow) %>% 
  arrange(desc(n))

most_observed_snow
```

The most common values are 0, NA, and 25. It make sense that most of months don't have snow. And NAs is a issue for snow.

```{r}
# Average max temperature in January and in July in each station across years
mean_tmax = noaa %>% 
  filter(month %in% c("Jan", "Jul")) %>% 
  group_by(year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = month)) +
  geom_point(alpha = .5) +
  geom_smooth(se = TRUE) +
  labs(title = "Average max temperature across year",
         x = "Year",
         y = "Average max temperature") +
  scale_x_continuous(
    breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)
    ) +
  facet_grid(. ~ month)

mean_tmax
```
  
By the plots, the most temperature lies around the line, which make sense that within a month, the temperature don't change dramatically. In January, the temperatures approximately are between -5 and 5, some points lies a little bit far from the line. In July, the average temperatures are from 25 to 30, temperatures are more tight than Jan. And there is no obvious outliers.

```{r}
# The plot showing tmax vs tmin
tmax_vs_tmin = noaa %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_smooth() +
  labs(title = "tmax vs. tmin",
         x = "Min temperature",
         y = "Max temperature")

tmax_vs_tmin  
```

From the plot, it is not very linear overall, more likely a s shape in the middle. Max temperatures are approximate from -20 to 45. And the min temperature are from -60 to 60. 

```{r}
# The plot showing snowfall values greater than 0 and less than 100 separately by year.
snowfall = noaa %>% 
  filter(snow > 0, snow < 100) %>% 
  mutate(year = as.factor(year)) %>% 
  ggplot(aes(x = snow, y = year, fill = year)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  labs(title = "Snowfall distribution by year",
         x = "Snowfall",
         y = "Year") 
snowfall
```

From the plot, the distributions are almost the same by years. And the values from 0 to 40 are the most popular.

```{r, fig.width=6, fig.height=4}
# Two-panel plot
library(patchwork)
tmax_vs_tmin + snowfall
```

The left is the plot showing tmax vs tmin, and the right is plot showing snowfall values greater than 0 and less than 100 separately by year. 


